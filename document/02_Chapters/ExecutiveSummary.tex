%%% File encoding is ISO-8859-1 (also known as Latin-1)
%%% You can use special characters just like ä,ü and ñ

% Chapter without numbering but with appearance in the Table of Contents
% \addchap is a command from KOMA-Script
\addchap{Executive Summary}

With the advent of the internet, the world faced an organizational crisis. What should be done with this monolith of textual information? In what ways might all this information be useful? Most importantly, how could this information be made accessible to the general public? These questions emphasized the need for search engines, programs that can troll through large masses of web content and pick out only the relevant bits of information in real time. Search engines have since become a staple and keystone for day-to-day life in the twenty-first century. It is now borderline instinctive to respond to any question with ``Just Google it!'' and to expect an answer within a few milliseconds.
\\\\
Despite its fundamental importance, search is complicated. It leans heavily on the subject of text mining---the extraction of pertinent information from natural language documents---a subject which faces many challenges both abstract and practical. How can one model the content of a textual document? Many documents use the same words, but each document may use the words differently. There is a combinatorial explosion of possibilities. What determines a document's semantics? What about semantic similarity? Most importantly, how can one measure these semantic features in real time? It turns out that many of these questions can be addressed at least in part by machine learning. In particular, the issue of semantic categorization can be reduced to a multiple-classification problem, one that is accurately solved by support vector machines.
\\\\
We address these questions in the present document, namely the questions of textual document representation and classification of documents into predefined semantic categories. We will discuss methods for representing textual data, isolating the semantics of a textual document, and predicting attributes of new documents with learned classifiers. Specifically, we will discuss: term-frequency/inverse-document-frequency representations of text and measures of similarity between different texts; semantic noise reduction of many documents via latent semantic analysis; unsupervised categorization of textual documents using k-nearest-neighbors; and supervised categorization of textual documents using support vector machines. At the end, the reader will be tasked with building a simple search engine, whose implementation will depend on the methods of text mining discussed in the background. 