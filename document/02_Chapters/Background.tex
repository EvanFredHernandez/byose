%%% File encoding is ISO-8859-1 (also known as Latin-1)
%%% You can use special characters just like ä,ü and ñ

\chapter{Background}

\section{Problem Description}
Consider a law firm that wants to build a strong defense for its client: thousands of legal documents are available to the firm, but only a small fraction of the information contained in these documents is useful for the given case. How would lawyers go about gathering pertinent information or evidence? On one hand, the lawyers could read every word in every document available and then hope to remember it all by the end. On the other hand, such brute force is inhumanly difficult and exorbitantly expensive. More to the point, not all available information is useful. In fact, chances are that the vast majority of legal documents are irrelevant to the case at hand. For example, if the lawyers are handling a case on tax evasion, they probably do not want to read about domestic laws. The firm ultimately needs an automated and highly efficient method for organizing textual information and for retrieving the pertinent information quickly.
\\\\
This problem of organizing textual information is not new. Humans began writing somewhere around 5000 B.C. and since then have created a monolithic amount of written information. For hundreds of years, the only way to research pertinent information was to visit a library and page through book after book. The process was slow and oftentimes unfruitful. Then, with the advent of the internet, the world rejoiced at the new availability of information and yet faced an organizational crisis. What should be done with this monolith of textual information? In what ways might all this information be useful? Most importantly, how could this information be made accessible to the general public? These questions emphasized the need for search engines, programs that could troll through large masses of web content and pick out only the relevant bits of information in real time. During the 90s, companies like Yahoo!, Ask Jeeves, and eventually Google sprang up, each offering unique methods for indexing webpages and providing speedy information retrieval. Search engines have since become a staple and keystone for day-to-day life in the twenty-first century.
\\\\
Underpinning much of search is the concept of \textbf{text mining}. Text mining refers to the organization and extraction of information from natural language documents. Note that search and text mining are not one and the same. Search engines typically undertake a number of steps to produce results, including indexing and query enrichment. By contrast, text mining is a more general collection of methods for extracting data from textual documents. Some examples of text mining include part-of-speech tagging, concept extraction, and sentiment analysis. In this paper, we will focus on the representation of textual documents as vectors and the categorization of textual documents into predefined categories. We will see that, with the right precautions, document categorization works out to be a fairly straightforward classification problem.
\\\\
Document categorization has many immediate applications. Lawyers may wish to automatically sort large quantities of text into semantic categories (domestic law, tax law, etc.); companies may wish to categorize incoming email and eliminate spam; search engines may want to categorize new queries to restrict their search space; and so forth. Despite its usefulness, document categorization is not trivial. It is not immediately obvious what determines whether a document belongs to a category. Language is by nature ambiguous. Some studies from cognitive science suggest that this ambiguity is evolutionarily advantageous because it eases the language learning process for young children, but that ease does not surface for computers. In particular, polysemy---the presence of multiple meanings for the same word---makes it difficult for computers to disambiguate between pairs of sentences like ``I climbed the steep \underline{bank},'' and ``I deposited a check in the \underline{bank}.'' On the other hand, language is often redundant and dilutes information with unnecessary clarifiers like pronouns and tense. Worse yet, the categorizer may have limited access to information about the documents. In the case of web search engines, the webpages will not come with category labels; it is up to the search engine to organize the data into semantic categories.
\\\\
To address these problems, we need an effective way to encode the semantics of a document without getting lost in the ambiguity and noisy redundancy of natural language. We present methods for doing so in the proceeding section.

\section{Representation of Textual Data}
In order to extract information from documents, we must first define what we mean by document. For our purposes, a \textbf{document} is a sequence of natural language tokens. In this sense, the sentence ``I ate an apple'' is as much a document as the entire play \textit{Hamlet}. It is worth noting that although our definition of document is limited to text, other definitions are typically more general so as to include any collection of information in written or electronic form. In fact, many of the methods for text document representation and categorization that we discuss here will apply to other types of documents as well.
\\\\
As one might expect, raw strings are not conducive to mathematical manipulation and therefore are not conducive to large-scale search. Performing string comparisons is both slow and inaccurate; the user is looking for a document semantically similar to his query, not for an exact textual match to his query. The alternative is to represent documents as vectors by defining a global list of features, and then encoding each document in terms of these features. Thus, for a corpus with $m$ features, each document will be encoded as an $m$-dimensional vector. The list of features is typically determined by the characteristics of the entire corpus. 
\\\\
Here, we will employ the term-document model. In the term-document model, each feature corresponds to a term from the collection of all terms taken over all documents. Its value is given by the number of times that term appears in the document. For example, let $d = [d_1 \ d_2 \ \dots \ d_m]^T$ be the feature vector given by some new document, and $\mathcal{V}$ the collection of all terms (features) in the corpus. If term $v \in \mathcal{V}$ appears in the document $k$ times, we have $d_v = k$, noting that we let each term correspond to some index of the vector. Unfortunately, this encoding is still meaningless: a document might contain the word ``the'' in 1256 different places, giving it a large feature value, but that does not mean the word ``the'' is semantically salient. As a first step to remedying this, we need to weight each term inversely to how often it is used throughout the corpus, under the assumption that rarer words better identify the semantics of a document. This encoding is called the \textbf{term frequency-inverse document frequency (tf-idf)} because each term frequency feature is multiplied by the inverse of its corpus frequency. 
\\\\
This encoding gives rise to a natural similarity measure for documents, namely, the normalized dot product or cosine similarity measure. For documents $d_i$ and $d_j$ in the same corpus, the similarity of $d_i$ and $d_j$ is given by $\frac{d_i^{\intercal}d_j}{||d_i||\cdot||d_j||}$. Any features that are not shared by both documents will be zeroed out, so the result is entirely determined by tf-idf values for terms that appear in both documents. Moreover, since common terms will have low weights, they will contribute little to the dot product, while shared rare terms like ``interpolate'' or ``Nietzsche'' will contribute a great deal to the dot product. This is in line with our assumption that rare words probably have semantic salience. Note that we can also obtain a measure of term similarity in this way: if we arrange each document vector as a row in a matrix, we see that the columns form feature vectors for terms, where the $i^{th}$ feature is the tf-idf of that term in document $i$. This is an intuitive representation because terms that are semantically similar will likely appear in the same document. As one might expect, the normalized dot product of two term vectors gives a rough similarity measure of those terms. 
\\\\
The term-document model is good at highlighting the semantic hotspots of a document---that is, the rare and potentially salient words---but it does not completely remove the redundancy of natural language. We must take other steps to reduce the noise from each document. One cheap solution is to remove short words from the feature space. This works because most short words are either articles or acronyms that are spelled out elsewhere in the text; however, there is still the risk of eliminating important information. Supposing we chose to remove all words of length three or less, say, in an effort to remove ``the'' from consideration, we might in the process eliminate the word ``bee'' from a text on endangered species, which is clearly undesirable. A more effective approach is to remove all \textbf{stop words} from the corpus, where a stop word is defined to be any word that contributes no information to the text, like ``the.'' The collection of stop words varies from case to case.
\\\\
Finally, before featurizing a corpus, we should make sure to remove any unnecessary prefixes or suffixes from each word, so that the words ``walk'' and ``walked'' are not considered separate features. This is called \textbf{stemming}. We omit the details.

\section{Latent Semantic Analysis}

In the previous section, we hinted that an entire corpus can be represented as a single matrix, where each row is a document vector. In fact, this representation exposes information about the corpus as a whole, and provides a means for extracting and simplifying that information.
\\\\
The astute reader will have noticed that the term-document model results in sparse vectors with many dimensions. Some of these dimensions are virtually useless, while others match the semantic category of a document but are zeroed out because the term represented by that dimension does not appear in the document. Despite our efforts at normalization with the term-document model, we still have not accounted for polysemy or synonomy. We want to uncover the latent semantic structures that underly a corpus. In other words, we would like to discover the semantic relationships between terms and to combine synonyms such that our term vectors (the columns of the corpus matrix) represent something closer to a semantic category than a specific word. For example, ``chihuahua'' and ``dog'' are different words but both belong to the same semantic category. From a search perspective, we would like to consider a document that mentions chihuahuas as similar to a query for dogs. 
\\\\
We can achieve this dimensionality reduction by performing \textbf{latent semantic analysis (LSA)} on the corpus matrix. Latent semantic analysis involves finding a low-rank approximation of the corpus matrix. To do this, we can apply the Eckart-Young, which states that the best $k$-rank approximation for an $n \times m$ matrix, $k < m$, over some unitarily invariant norm is given by the outer product of the the first $k$ singular values and vectors of the original matrix. Formally, if the singular value decomposition of a matrix $A$ is given by $\sum_{i=1}^r\sigma_iu_iv_i^{\intercal}$, then the best $k$-rank approximation to $A$ is
	$$A_k = \sum_{i=1}^k\sigma_iu_iv_i^{\intercal}$$
for $k < r$. We can think of this approximation as zeroing out terms that are not salient to the corpus, though the mathematical details are much finer than simply making a row or column zero. This helps to highlight  
\\\\
In some cases, it is desirable to do this by clipping the matrix dimension to $n \times k$, where $k < m$ and $m$ is the number of terms in the corpus vocabulary.

\section{Categorization of Textual Documents}
We have finally arrived at the point where we can discuss the methods of classification. Through classification, we hope to determine which classes a new data entry belongs to. To do so, we require a classifier, a model learned from training data. Various classifiers exist, each with their own advantages and disadvantages. In this section, we will be discussing and comparing the least squares, k-nearest neighbors, and support vector machine classifiers. One thing to note is that we are dealing with binary (two-class) data: there are as many classes as dimensions. In practice, two types of multi-class classification are used for least squares and support vector machines: one versus one and one versus all. In one versus all, one classifier is learned that determines whether the data is in that category or not. In one versus one, a classifier is built to compare each pair of classes, resulting in K(K-1)/2 classifiers which is far more computationally expensive. However, the advantage to one versus one classification is that it accounts better for overlapping categories than one versus all thus providing better classification accuracy. Because data can have multiple classes in the text classification, a new document can take the categories of the top k categories, or all categories above a given threshold.
\\\\
The first classifier that we will inspect is the least squares classifier, a supervised learning method that takes an overdetermined system (lots of data, noise present) and minimizes over the sum of squared differences between the given categories and the corresponding modeled values, computed by multiplying the data feature matrix by a weight vector. 
\\\\
A method of solving the least squares problem is provided through the pseudoinverse, which relies on the singular value decomposition of a matrix.
\\\\
Least squares has great performance when it comes to linearly separable data, but this is often not the case in the real world. In fact, often times there is noise in addition to outliers, and this is where the least squares classifier suffers because it tries to minimize classification error, so it takes into account extreme outliers and tries to minimize that error too. 
\\\\
The second classifier we shall discuss is the k-nearest neighbors classifier. This classifiers chooses the k points from the training data that are closest (in euclidean distance) to the new entry and classifies it based off of the majority vote. In reference to the figure above, if k = 3, then the new point would be classified as red since 2 of its 3 closest neighbors are red; if k = 5, it would be blue. This method works quite well for text classification. In fact, it is the most commonly used technique for a number of reasons. The first is that it is quite simple to implement. Another benefit is that it requires no training or precomputation: all of the computation occurs during classification. However, this is also quite a disadvantage, because every new data entry must be compared to every training data member in the classifier, which can be slow with large amounts of data in large feature spaces, as is the case with text classification. Another drawback is that results vary as you change k. In practice, k is determined to be ?elbow? of the graph comparing k to classification error. But to create this graph, every member of the training data needs to be classified for each value of k, which is quite costly.
\\\\
Finally, we have arrived at the support vector machine (SVM). The support vector margin hopes to define the hyperplane in the feature space that produces the greatest margin between the two classes. However, there is often noise, producing overlap in the feature space, so support vector machines instead try to minimize classification error, which can be done iteratively. After the classifier is trained, it computes the classification of a new data entry by taking the sign of the inner product of the learned weight vector and the data vector. 
\\\\
We have ranked the SVM as the best classifier for our text classification problem and we have various reasons for that. When dealing with text classifiers, each word in the text is an entry in the vector of text features. With large text files, we are dealing with feature vectors with thousands of features. When this occurs, many problems can arise that cannot be handled properly by least squares and KNN classification such as overfitting and vector sparsity. By applying the SVM to this problem, however, we can mostly void these issues. First of all, the SVM maximizes the margin space between each data set. This process is independent of the dimensionality of the feature space. Therefore, it helps us avoid the issue of overfitting. Secondly, sparse data does not adversely affect the SVM. One way to see this is to do a random rotation of the coordinate axes, which would leave the problem itself unchanged and give the same solution, but would make the data non-sparse.